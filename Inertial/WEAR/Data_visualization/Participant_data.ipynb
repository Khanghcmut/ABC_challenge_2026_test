{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38887e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_50022/4209097086.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/core/arrays/arrow/array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/Users/khangphan/.pyenv/versions/3.10.5/lib/python3.10/site-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np # Added numpy just in case it's used elsewhere\n",
    "\n",
    "# Suppress warnings that may arise during plotting/stitching\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95302f",
   "metadata": {},
   "source": [
    "1. Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106a239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Configurations ---\n",
    "# Define directory paths ( Để lấy dữ liệu của participant và save vào mục file chứa file xử lý png của participants)\n",
    "DATA_DIR: str = '../Processed_data_shrink'\n",
    "OUTPUT_BASE_DIR: str = './image list'\n",
    "ACTIVITY_OUTPUT_FILE: str = 'overlaid_chronological_activities.png'\n",
    "\n",
    "# Define Activity Labels \n",
    "LABEL_MAP: Dict[int, str] = {\n",
    "    1: 'jogging', 2: 'jogging (rotating arms)', 3: 'jogging (skipping)',\n",
    "    4: 'jogging (sidesteps)', 5: 'jogging (butt-kicks)', 6: 'stretching (triceps)',\n",
    "    7: 'stretching (lunging)', 8: 'stretching (shoulders)', 9: 'stretching (hamstrings)',\n",
    "    10: 'stretching (lumbar rotation)', 11: 'push-ups', 12: 'push-ups (complex)',\n",
    "    13: 'sit-ups', 14: 'sit-ups (complex)', 15: 'burpees',\n",
    "    16: 'lunges', 17: 'lunges (complex)', 18: 'bench-dips',\n",
    "    19: 'Not recognized'\n",
    "}\n",
    "# Creates the inverse map: Activity Name -> Code\n",
    "INVERTED_LABEL_MAP: Dict[str, int] = {v: k for k, v in LABEL_MAP.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab9b57",
   "metadata": {},
   "source": [
    "2. Activity Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6f0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generic Helper Functions ---\n",
    "\n",
    "def _prepare_activity_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Helper function to clean and prepare the 'label' column for plotting.\"\"\"\n",
    "    if df['label'].dtype == object:\n",
    "        df['label'] = df['label'].map(INVERTED_LABEL_MAP)\n",
    "    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "    return df.dropna(subset=['label'])\n",
    "\n",
    "\n",
    "def _plot_overlaid_activity(df_train: pd.DataFrame, df_test: pd.DataFrame) -> str:\n",
    "    \"\"\"Generates a single plot showing training and test activities overlaid (sensor-agnostic).\"\"\"\n",
    "    \n",
    "    df_train_plot = _prepare_activity_data(df_train.copy())\n",
    "    df_test_plot = _prepare_activity_data(df_test.copy())\n",
    "\n",
    "    fig_act, ax_act = plt.subplots(1, 1, figsize=(15, 6))\n",
    "    \n",
    "    # Plot Training Activity\n",
    "    ax_act.plot(df_train_plot.index, df_train_plot['label'], drawstyle='steps-post', label='Training Activity', linewidth=1.5, color='C2')\n",
    "    # Plot Test Activity (Overlay)\n",
    "    ax_act.plot(df_test_plot.index, df_test_plot['label'], drawstyle='steps-post', label='Test Activity', linewidth=1.5, color='C3', alpha=0.7) \n",
    "\n",
    "    activity_labels = list(LABEL_MAP.values())\n",
    "    activity_values = list(LABEL_MAP.keys())\n",
    "\n",
    "    ax_act.set_yticks(activity_values)\n",
    "    ax_act.set_yticklabels(activity_labels)\n",
    "    ax_act.set_xlabel('Time Step (Index) - Chronological Order')\n",
    "    ax_act.set_ylabel('Activity')\n",
    "    ax_act.set_title('Chronological Activities: Training and Test Overlaid', fontsize=14)\n",
    "    ax_act.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax_act.legend(loc='upper right')\n",
    "\n",
    "    ax_act.set_ylim(min(activity_values) - 0.5, max(activity_values) + 0.5)\n",
    "    max_len = max(len(df_train_plot), len(df_test_plot))\n",
    "    ax_act.set_xlim(0, max_len)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_act.savefig(ACTIVITY_OUTPUT_FILE)\n",
    "    plt.close(fig_act)\n",
    "    return ACTIVITY_OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4634129",
   "metadata": {},
   "source": [
    "3. Image Stitching and Cleanup\n",
    "Tác dụng: gộp 3 graph acceleration x,y,z của tay (và chân) với activities graph thành 1 file png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98563516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stitch_images(file_list: List[str], final_output_file: str, output_dir: str) -> str:\n",
    "    \"\"\"Stitches multiple PNG images, saves the final file to the specified directory, and cleans up the intermediary files.\"\"\"\n",
    "    \n",
    "    final_output_path = os.path.join(output_dir, final_output_file)\n",
    "    \n",
    "    if not file_list or any(not os.path.exists(f) for f in file_list):\n",
    "        for f in file_list:\n",
    "            if os.path.exists(f):\n",
    "                os.remove(f)\n",
    "        raise FileNotFoundError(\"One or more generated plot files are missing. Stitching aborted.\")\n",
    "        \n",
    "    images = [Image.open(f) for f in file_list]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    max_width = max(widths)\n",
    "    total_height = sum(heights)\n",
    "\n",
    "    combined_img = Image.new('RGB', (max_width, total_height), color='white')\n",
    "\n",
    "    y_offset = 0\n",
    "    for img in images:\n",
    "        x_offset = int((max_width - img.size[0]) / 2)\n",
    "        combined_img.paste(img, (x_offset, y_offset))\n",
    "        y_offset += img.size[1]\n",
    "\n",
    "    # Create directory if it doesn't exist and save the final image\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    combined_img.save(final_output_path)\n",
    "    \n",
    "    # Delete individual plot files from the current working directory\n",
    "    for f in file_list:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "    \n",
    "    return final_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f650bf",
   "metadata": {},
   "source": [
    "3.1. Arm Acceleration Plotting\n",
    "Plotting arm acceleration data của training và test của cùng 1 participant và xuất file png (bằng function _stitch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cda900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Arm Acceleration Functions ---\n",
    "\n",
    "def _plot_separate_arm_acceleration(df_train: pd.DataFrame, df_test: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Generates three temporary figures for X, Y, Z ARM acceleration.\"\"\"\n",
    "    \n",
    "    acc_cols = [\"arm_acc_x\", \"arm_acc_y\", \"arm_acc_z\"]\n",
    "    generated_files = []\n",
    "    \n",
    "    for col in acc_cols:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "        ax.plot(df_train.index, df_train[col], label='Training Data', linewidth=0.5, color='C0')\n",
    "        ax.plot(df_test.index, df_test[col], label='Test Data', linewidth=0.5, color='C1', alpha=0.7)\n",
    "\n",
    "        axis = col.split(\"_\")[-1].upper()\n",
    "        ax.set_title(f'Arm Acceleration Component: {axis} Axis - Training vs. Test (Overlay)', fontsize=14)\n",
    "        ax.set_ylabel(r'Acceleration ($\\mathrm{m/s^2}$)')\n",
    "        ax.set_xlabel('Time Step (Index) - Chronological Order')\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.legend(loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_filename = f'combined_arm_acc_{axis.lower()}.png'\n",
    "        fig.savefig(output_filename)\n",
    "        plt.close(fig)\n",
    "        generated_files.append(output_filename)\n",
    "        \n",
    "    return generated_files\n",
    "\n",
    "\n",
    "def generate_combined_arm_acceleration_plots(df_train: pd.DataFrame, df_test: pd.DataFrame, output_dir: str) -> str:\n",
    "    \"\"\"Generates and saves the final combined PNG for ARM acceleration.\"\"\"\n",
    "    \n",
    "    all_generated_files = []\n",
    "    \n",
    "    all_generated_files.extend(_plot_separate_arm_acceleration(df_train, df_test))\n",
    "    \n",
    "    # We must include the activity plot path to stitch it and ensure it's deleted.\n",
    "    if not os.path.exists(ACTIVITY_OUTPUT_FILE):\n",
    "        all_generated_files.append(_plot_overlaid_activity(df_train.copy(), df_test.copy()))\n",
    "    else:\n",
    "        all_generated_files.append(ACTIVITY_OUTPUT_FILE)\n",
    "\n",
    "    final_file = \"all_graphs_combined_arm.png\"\n",
    "    return _stitch_images(all_generated_files, final_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2090ab8",
   "metadata": {},
   "source": [
    "3.2 Leg Acceleration Plotting\n",
    "Plotting leg acceleration data của training và test của cùng 1 participant và xuất file png (bằng function _stitch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7db7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Leg Acceleration Functions ---\n",
    "\n",
    "def _plot_separate_leg_acceleration(df_train: pd.DataFrame, df_test: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Generates three temporary figures for X, Y, Z LEG acceleration.\"\"\"\n",
    "    \n",
    "    acc_cols = [\"leg_acc_x\", \"leg_acc_y\", \"leg_acc_z\"]\n",
    "    generated_files = []\n",
    "    \n",
    "    for col in acc_cols:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "        ax.plot(df_train.index, df_train[col], label='Training Data', linewidth=0.5, color='C0')\n",
    "        ax.plot(df_test.index, df_test[col], label='Test Data', linewidth=0.5, color='C1', alpha=0.7)\n",
    "\n",
    "        axis = col.split(\"_\")[-1].upper()\n",
    "        ax.set_title(f'Leg Acceleration Component: {axis} Axis - Training vs. Test (Overlay)', fontsize=14)\n",
    "        ax.set_ylabel(r'Acceleration ($\\mathrm{m/s^2}$)')\n",
    "        ax.set_xlabel('Time Step (Index) - Chronological Order')\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.legend(loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_filename = f'combined_leg_acc_{axis.lower()}.png'\n",
    "        fig.savefig(output_filename)\n",
    "        plt.close(fig)\n",
    "        generated_files.append(output_filename)\n",
    "        \n",
    "    return generated_files\n",
    "\n",
    "\n",
    "def generate_combined_leg_acceleration_plots(df_train: pd.DataFrame, df_test: pd.DataFrame, output_dir: str) -> str:\n",
    "    \"\"\"Generates and saves the final combined PNG for LEG acceleration.\"\"\"\n",
    "    \n",
    "    all_generated_files = []\n",
    "    \n",
    "    all_generated_files.extend(_plot_separate_leg_acceleration(df_train, df_test))\n",
    "    all_generated_files.append(_plot_overlaid_activity(df_train.copy(), df_test.copy()))\n",
    "\n",
    "    final_file = \"all_graphs_combined_leg.png\"\n",
    "    return _stitch_images(all_generated_files, final_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d739cff",
   "metadata": {},
   "source": [
    "4. Thực hiện quả trình gộp, xuất file png cho từng participant và save trong folder cá nhân  của participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a83785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Master Execution Function ---\n",
    "\n",
    "def process_all_participants() -> str:\n",
    "    \"\"\"\n",
    "    Finds all participant data pairs, creates an output folder for each,\n",
    "    and generates the Arm and Leg combined plots, saving them to the participant's folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(DATA_DIR):\n",
    "            return f\"\\nERROR: Data directory not found: '{DATA_DIR}'. Please ensure it exists.\"\n",
    "\n",
    "        # --- 1. Identify and Group Files ---\n",
    "        all_files = os.listdir(DATA_DIR)\n",
    "        \n",
    "        # Regex captures data type (1 or 2) and subject code (e.g., 0, 1, 0_2)\n",
    "        pattern = re.compile(r'([12])_sbj_([0-9_]+)\\.csv')\n",
    "        \n",
    "        file_groups: Dict[str, Dict[str, str]] = {}\n",
    "\n",
    "        for filename in all_files:\n",
    "            match = pattern.match(filename)\n",
    "            if match:\n",
    "                data_type, subj_code = match.groups()\n",
    "                if subj_code not in file_groups:\n",
    "                    file_groups[subj_code] = {}\n",
    "                # '1' is training data path, '2' is test data path\n",
    "                file_groups[subj_code][data_type] = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "        # Filter for only subjects that have both a '1' (train) and '2' (test) file\n",
    "        complete_groups = {\n",
    "            subj: paths for subj, paths in file_groups.items() \n",
    "            if '1' in paths and '2' in paths\n",
    "        }\n",
    "        \n",
    "        if not complete_groups:\n",
    "            return f\"\\nNo complete participant pairs ('1_sbj_X.csv' and '2_sbj_X.csv') found in '{DATA_DIR}'. Found {len(all_files)} files.\"\n",
    "\n",
    "        # --- 2. Process Each Participant ---\n",
    "        print(f\"Found {len(complete_groups)} complete participant pairs to process.\")\n",
    "        summary_results: List[str] = []\n",
    "        \n",
    "        for subj_code, paths in complete_groups.items():\n",
    "            print(f\"\\n--- Processing Participant Code: {subj_code} ---\")\n",
    "            train_path = paths['1']\n",
    "            test_path = paths['2']\n",
    "            \n",
    "            # The folder name is the participant's code name\n",
    "            output_folder = os.path.join(OUTPUT_BASE_DIR, subj_code)\n",
    "\n",
    "            # Load Data\n",
    "            df_train = pd.read_csv(train_path)\n",
    "            df_test = pd.read_csv(test_path)\n",
    "            \n",
    "            # Generate and save LEG analysis\n",
    "            print(f\"  -> Generating LEG analysis for {subj_code}...\")\n",
    "            leg_file_path = generate_combined_leg_acceleration_plots(df_train, df_test, output_folder)\n",
    "            summary_results.append(f\"✅ Leg plots for {subj_code} saved to: {leg_file_path}\")\n",
    "\n",
    "            # Generate and save ARM analysis\n",
    "            print(f\"  -> Generating ARM analysis for {subj_code}...\")\n",
    "            arm_file_path = generate_combined_arm_acceleration_plots(df_train, df_test, output_folder)\n",
    "            summary_results.append(f\"✅ Arm plots for {subj_code} saved to: {arm_file_path}\")\n",
    "\n",
    "        return \"\\n\" + \"\\n\".join(summary_results) + \"\\n\\n**Processing complete for all participants.**\"\n",
    "\n",
    "    except KeyError as e:\n",
    "        return f\"\\nERROR: Missing required data column: {e}. Please check the CSV file structure.\"\n",
    "    except Exception as e:\n",
    "        return f\"\\nAn unexpected error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7564e",
   "metadata": {},
   "source": [
    "5. Chạy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbf25d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 complete participant pairs to process.\n",
      "\n",
      "--- Processing Participant Code: 0_2 ---\n",
      "  -> Generating LEG analysis for 0_2...\n",
      "  -> Generating ARM analysis for 0_2...\n",
      "\n",
      "--- Processing Participant Code: 2 ---\n",
      "  -> Generating LEG analysis for 2...\n",
      "  -> Generating ARM analysis for 2...\n",
      "\n",
      "--- Processing Participant Code: 0 ---\n",
      "  -> Generating LEG analysis for 0...\n",
      "  -> Generating ARM analysis for 0...\n",
      "\n",
      "--- Processing Participant Code: 1 ---\n",
      "  -> Generating LEG analysis for 1...\n",
      "  -> Generating ARM analysis for 1...\n",
      "\n",
      "✅ Leg plots for 0_2 saved to: ./image list/0_2/all_graphs_combined_leg.png\n",
      "✅ Arm plots for 0_2 saved to: ./image list/0_2/all_graphs_combined_arm.png\n",
      "✅ Leg plots for 2 saved to: ./image list/2/all_graphs_combined_leg.png\n",
      "✅ Arm plots for 2 saved to: ./image list/2/all_graphs_combined_arm.png\n",
      "✅ Leg plots for 0 saved to: ./image list/0/all_graphs_combined_leg.png\n",
      "✅ Arm plots for 0 saved to: ./image list/0/all_graphs_combined_arm.png\n",
      "✅ Leg plots for 1 saved to: ./image list/1/all_graphs_combined_leg.png\n",
      "✅ Arm plots for 1 saved to: ./image list/1/all_graphs_combined_arm.png\n",
      "\n",
      "**Processing complete for all participants.**\n"
     ]
    }
   ],
   "source": [
    "# Execute the master function\n",
    "result = process_all_participants()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
